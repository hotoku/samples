{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ebfc963-3ba3-45a5-aa7c-c38f6a981722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.9/441.9 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp310-cp310-macosx_11_0_arm64.whl (31 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (2.28.1)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.0-cp310-cp310-macosx_11_0_arm64.whl (22.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.6/22.6 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (1.4.4)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting dill<0.3.6\n",
      "  Using cached dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.2.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from datasets) (1.23.3)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp310-cp310-macosx_11_0_arm64.whl (336 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.4/336.4 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.8.1-cp310-cp310-macosx_11_0_arm64.whl (57 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.1-cp310-cp310-macosx_11_0_arm64.whl (34 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.2-cp310-cp310-macosx_11_0_arm64.whl (29 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.13-py310-none-any.whl (133 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, tqdm, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.6.1 dill-0.3.5.1 frozenlist-1.3.1 fsspec-2022.10.0 huggingface-hub-0.10.1 multidict-6.0.2 multiprocess-0.70.13 pyarrow-10.0.0 responses-0.18.0 tqdm-4.64.1 xxhash-3.1.0 yarl-1.8.1\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/Users/hotoku/.pyenv/versions/3.10.5/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51b63cc-e0ac-41b0-b17b-94bc8b54b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/Users/hotoku/.pyenv/versions/3.10.5/bin/python3.10 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18109b1b-772c-4dc9-94c2-9b0e0380bfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hotoku/.pyenv/versions/3.10.5/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f55857c-0817-4102-b8bf-c8b4ca44aff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset snow_simplified_japanese_corpus/snow_t15 to /Users/hotoku/.cache/huggingface/datasets/snow_simplified_japanese_corpus/snow_t15/1.1.0/3d2b3ae03002b35ba284fd81fe825917859e4365c825af4ab3f10273074c81f6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.63M/3.63M [00:05<00:00, 679kB/s] \n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset snow_simplified_japanese_corpus downloaded and prepared to /Users/hotoku/.cache/huggingface/datasets/snow_simplified_japanese_corpus/snow_t15/1.1.0/3d2b3ae03002b35ba284fd81fe825917859e4365c825af4ab3f10273074c81f6. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 393.09it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"snow_simplified_japanese_corpus\", \"snow_t15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b0ae07-a53a-4af6-ab5a-6c3a911d6113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddb45ec-160f-4d98-b8cc-5320896db8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'original_ja', 'simplified_ja', 'original_en'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8daae0f9-7fc0-4e2b-b0e1-414a13bc6de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': Value(dtype='string', id=None),\n",
       " 'original_ja': Value(dtype='string', id=None),\n",
       " 'simplified_ja': Value(dtype='string', id=None),\n",
       " 'original_en': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95e23bed-9c6e-4ec5-b562-bfd1d0e51a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['誰が一番に着くか私には分かりません。', '多くの動物が人間によって滅ぼされた。', '私はテニス部員です。']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][\"original_ja\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ada24d6-f1c9-403b-8cb3-a76c09543c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Dataset in module datasets.arrow_dataset object:\n",
      "\n",
      "class Dataset(DatasetInfoMixin, datasets.search.IndexableMixin, TensorflowDatasetMixin)\n",
      " |  Dataset(arrow_table: datasets.table.Table, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_table: Optional[datasets.table.Table] = None, fingerprint: Optional[str] = None)\n",
      " |  \n",
      " |  A Dataset backed by an Arrow table.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dataset\n",
      " |      DatasetInfoMixin\n",
      " |      datasets.search.IndexableMixin\n",
      " |      TensorflowDatasetMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\n",
      " |  \n",
      " |  __init__(self, arrow_table: datasets.table.Table, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_table: Optional[datasets.table.Table] = None, fingerprint: Optional[str] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterate through the examples.\n",
      " |      \n",
      " |      If a formatting is set with :meth:`Dataset.set_format` rows will be returned with the\n",
      " |      selected format.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Number of rows in the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.__len__\n",
      " |      <bound method Dataset.__len__ of Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 1066\n",
      " |      })>\n",
      " |      ```\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_column(self, name: str, column: Union[list, <built-in function array>], new_fingerprint: str)\n",
      " |      Add column to Dataset.\n",
      " |      \n",
      " |      *New in version 1.7.*\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): Column name.\n",
      " |          column (list or np.array): Column data to be added.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> more_text = ds[\"text\"]\n",
      " |      >>> ds.add_column(name=\"text_2\", column=more_text)\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label', 'text_2'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  add_elasticsearch_index(self, column: str, index_name: Optional[str] = None, host: Optional[str] = None, port: Optional[int] = None, es_client: Optional[ForwardRef('elasticsearch.Elasticsearch')] = None, es_index_name: Optional[str] = None, es_index_config: Optional[dict] = None)\n",
      " |      Add a text index using ElasticSearch for fast retrieval. This is done in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |              The column of the documents to add to the index.\n",
      " |          index_name (Optional :obj:`str`):\n",
      " |              The index_name/identifier of the index.\n",
      " |              This is the index name that is used to call :meth:`Dataset.get_nearest_examples` or :meth:`Dataset.search`.\n",
      " |              By default it corresponds to :obj:`column`.\n",
      " |          host (Optional :obj:`str`, defaults to localhost):\n",
      " |              host of where ElasticSearch is running\n",
      " |          port (Optional :obj:`str`, defaults to 9200):\n",
      " |              port of where ElasticSearch is running\n",
      " |          es_client (Optional :obj:`elasticsearch.Elasticsearch`):\n",
      " |              The elasticsearch client used to create the index if host and port are None.\n",
      " |          es_index_name (Optional :obj:`str`):\n",
      " |              The elasticsearch index name used to create the index.\n",
      " |          es_index_config (Optional :obj:`dict`):\n",
      " |              The configuration of the elasticsearch index.\n",
      " |              Default config is:\n",
      " |              ```\n",
      " |      \n",
      " |                  {\n",
      " |                      \"settings\": {\n",
      " |                          \"number_of_shards\": 1,\n",
      " |                          \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
      " |                      },\n",
      " |                      \"mappings\": {\n",
      " |                          \"properties\": {\n",
      " |                              \"text\": {\n",
      " |                                  \"type\": \"text\",\n",
      " |                                  \"analyzer\": \"standard\",\n",
      " |                                  \"similarity\": \"BM25\"\n",
      " |                              },\n",
      " |                          }\n",
      " |                      },\n",
      " |                  }\n",
      " |              ```\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> es_client = elasticsearch.Elasticsearch()\n",
      " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |      >>> ds.add_elasticsearch_index(column='line', es_client=es_client, es_index_name=\"my_es_index\")\n",
      " |      >>> scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)\n",
      " |      ```\n",
      " |  \n",
      " |  add_faiss_index(self, column: str, index_name: Optional[str] = None, device: Optional[int] = None, string_factory: Optional[str] = None, metric_type: Optional[int] = None, custom_index: Optional[ForwardRef('faiss.Index')] = None, batch_size: int = 1000, train_size: Optional[int] = None, faiss_verbose: bool = False, dtype=<class 'numpy.float32'>)\n",
      " |      Add a dense index using Faiss for fast retrieval.\n",
      " |      By default the index is done over the vectors of the specified column.\n",
      " |      You can specify :obj:`device` if you want to run it on GPU (:obj:`device` must be the GPU index).\n",
      " |      You can find more information about Faiss here:\n",
      " |      \n",
      " |      - For `string factory <https://github.com/facebookresearch/faiss/wiki/The-index-factory>`__\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`):\n",
      " |              The column of the vectors to add to the index.\n",
      " |          index_name (Optional :obj:`str`):\n",
      " |              The index_name/identifier of the index.\n",
      " |              This is the index_name that is used to call :func:`datasets.Dataset.get_nearest_examples` or :func:`datasets.Dataset.search`.\n",
      " |              By default it corresponds to `column`.\n",
      " |          device (Optional :obj:`Union[int, List[int]]`): If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
      " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      " |          string_factory (Optional :obj:`str`):\n",
      " |              This is passed to the index factory of Faiss to create the index.\n",
      " |              Default index class is ``IndexFlat``.\n",
      " |          metric_type (Optional :obj:`int`):\n",
      " |              Type of metric. Ex: faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2.\n",
      " |          custom_index (Optional :obj:`faiss.Index`):\n",
      " |              Custom Faiss index that you already have instantiated and configured for your needs.\n",
      " |          batch_size (Optional :obj:`int`): Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n",
      " |              <Added version=\"2.4.0\"/>\n",
      " |          train_size (Optional :obj:`int`):\n",
      " |              If the index needs a training step, specifies how many vectors will be used to train the index.\n",
      " |          faiss_verbose (:obj:`bool`, defaults to False):\n",
      " |              Enable the verbosity of the Faiss index.\n",
      " |          dtype (data-type): The dtype of the numpy arrays that are indexed.\n",
      " |              Default is ``np.float32``.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |      >>> ds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))\n",
      " |      >>> ds_with_embeddings.add_faiss_index(column='embeddings')\n",
      " |      >>> # query\n",
      " |      >>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
      " |      >>> # save index\n",
      " |      >>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n",
      " |      \n",
      " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
      " |      >>> # load index\n",
      " |      >>> ds.load_faiss_index('embeddings', 'my_index.faiss')\n",
      " |      >>> # query\n",
      " |      >>> scores, retrieved_examples = ds.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
      " |      ```\n",
      " |  \n",
      " |  add_faiss_index_from_external_arrays(self, external_arrays: <built-in function array>, index_name: str, device: Optional[int] = None, string_factory: Optional[str] = None, metric_type: Optional[int] = None, custom_index: Optional[ForwardRef('faiss.Index')] = None, batch_size: int = 1000, train_size: Optional[int] = None, faiss_verbose: bool = False, dtype=<class 'numpy.float32'>)\n",
      " |      Add a dense index using Faiss for fast retrieval.\n",
      " |      The index is created using the vectors of `external_arrays`.\n",
      " |      You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n",
      " |      You can find more information about Faiss here:\n",
      " |      \n",
      " |      - For `string factory <https://github.com/facebookresearch/faiss/wiki/The-index-factory>`__\n",
      " |      \n",
      " |      Args:\n",
      " |          external_arrays (:obj:`np.array`):\n",
      " |              If you want to use arrays from outside the lib for the index, you can set :obj:`external_arrays`.\n",
      " |              It will use :obj:`external_arrays` to create the Faiss index instead of the arrays in the given :obj:`column`.\n",
      " |          index_name (:obj:`str`):\n",
      " |              The index_name/identifier of the index.\n",
      " |              This is the index_name that is used to call :func:`datasets.Dataset.get_nearest_examples` or :func:`datasets.Dataset.search`.\n",
      " |          device (Optional :obj:`Union[int, List[int]]`): If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
      " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      " |          string_factory (Optional :obj:`str`):\n",
      " |              This is passed to the index factory of Faiss to create the index.\n",
      " |              Default index class is ``IndexFlat``.\n",
      " |          metric_type (Optional :obj:`int`):\n",
      " |              Type of metric. Ex: faiss.faiss.METRIC_INNER_PRODUCT or faiss.METRIC_L2.\n",
      " |          custom_index (Optional :obj:`faiss.Index`):\n",
      " |              Custom Faiss index that you already have instantiated and configured for your needs.\n",
      " |          batch_size (Optional :obj:`int`): Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n",
      " |              <Added version=\"2.4.0\"/>\n",
      " |          train_size (Optional :obj:`int`):\n",
      " |              If the index needs a training step, specifies how many vectors will be used to train the index.\n",
      " |          faiss_verbose (:obj:`bool`, defaults to False):\n",
      " |              Enable the verbosity of the Faiss index.\n",
      " |          dtype (:obj:`numpy.dtype`): The dtype of the numpy arrays that are indexed. Default is np.float32.\n",
      " |  \n",
      " |  add_item(self, item: dict, new_fingerprint: str)\n",
      " |      Add item to Dataset.\n",
      " |      \n",
      " |      *New in version 1.7.*\n",
      " |      \n",
      " |      Args:\n",
      " |          item (dict): Item data to be added.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> new_review = {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n",
      " |      >>> ds = ds.add_item(new_review)\n",
      " |      >>> ds[-1]\n",
      " |      {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n",
      " |      ```\n",
      " |  \n",
      " |  align_labels_with_mapping(self, label2id: Dict, label_column: str) -> 'Dataset'\n",
      " |      Align the dataset's label ID and label name mapping to match an input :obj:`label2id` mapping.\n",
      " |      This is useful when you want to ensure that a model's predicted labels are aligned with the dataset.\n",
      " |      The alignment in done using the lowercase label names.\n",
      " |      \n",
      " |      Args:\n",
      " |          label2id (:obj:`dict`):\n",
      " |              The label name to ID mapping to align the dataset with.\n",
      " |          label_column (:obj:`str`):\n",
      " |              The column name of labels to align on.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
      " |      >>> ds = load_dataset(\"glue\", \"mnli\", split=\"train\")\n",
      " |      >>> # mapping to align with\n",
      " |      >>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
      " |      >>> ds_aligned = ds.align_labels_with_mapping(label2id, \"label\")\n",
      " |      ```\n",
      " |  \n",
      " |  cast(self, features: datasets.features.features.Features, batch_size: Optional[int] = 10000, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 10000, num_proc: Optional[int] = None) -> 'Dataset'\n",
      " |      Cast the dataset to a new set of features.\n",
      " |      \n",
      " |      Args:\n",
      " |          features (:class:`datasets.Features`): New features to cast the dataset to.\n",
      " |              The name of the fields in the features must match the current column names.\n",
      " |              The type of the data must also be convertible from one type to the other.\n",
      " |              For non-trivial conversion, e.g. string <-> ClassLabel you should use :func:`map` to update the Dataset.\n",
      " |          batch_size (:obj:`int`, defaults to `1000`): Number of examples per batch provided to cast.\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to cast.\n",
      " |          keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n",
      " |          load_from_cache_file (:obj:`bool`, default `True` if caching is enabled): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_name (:obj:`str`, optional, default `None`): Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          num_proc (:obj:`int`, optional, default `None`): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`: A copy of the dataset with casted features.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset, ClassLabel, Value\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      >>> new_features = ds.features.copy()\n",
      " |      >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
      " |      >>> new_features['text'] = Value('large_string')\n",
      " |      >>> ds = ds.cast(new_features)\n",
      " |      >>> ds.features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " |       'text': Value(dtype='large_string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  cast_column(self, column: str, feature: Union[dict, list, tuple, datasets.features.features.Value, datasets.features.features.ClassLabel, datasets.features.translation.Translation, datasets.features.translation.TranslationVariableLanguages, datasets.features.features.Sequence, datasets.features.features.Array2D, datasets.features.features.Array3D, datasets.features.features.Array4D, datasets.features.features.Array5D, datasets.features.audio.Audio, datasets.features.image.Image], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Cast column to feature for decoding.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`): Column name.\n",
      " |          feature (:class:`FeatureType`): Target feature.\n",
      " |          new_fingerprint (:obj:`str`, optional): The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
      " |      >>> ds.features\n",
      " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
      " |       'text': Value(dtype='string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  class_encode_column(self, column: str, include_nulls: bool = False) -> 'Dataset'\n",
      " |      Casts the given column as :obj:``datasets.features.ClassLabel`` and updates the table.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (`str`): The name of the column to cast (list all the column names with :func:`datasets.Dataset.column_names`)\n",
      " |          include_nulls (`bool`, default `False`):\n",
      " |              Whether to include null values in the class labels. If True, the null values will be encoded as the `\"None\"` class label.\n",
      " |      \n",
      " |              *New in version 1.14.2*\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"boolq\", split=\"validation\")\n",
      " |      >>> ds.features\n",
      " |      {'answer': Value(dtype='bool', id=None),\n",
      " |       'passage': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None)}\n",
      " |      >>> ds = ds.class_encode_column('answer')\n",
      " |      >>> ds.features\n",
      " |      {'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n",
      " |       'passage': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None)}\n",
      " |      ```\n",
      " |  \n",
      " |  cleanup_cache_files(self) -> int\n",
      " |      Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is\n",
      " |      one.\n",
      " |      \n",
      " |      Be careful when running this command that no other process is currently using other cache files.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`int`: Number of removed files.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.cleanup_cache_files()\n",
      " |      10\n",
      " |      ```\n",
      " |  \n",
      " |  export(self, filename: str, format: str = 'tfrecord')\n",
      " |      Writes the Arrow dataset to a TFRecord file.\n",
      " |      \n",
      " |      The dataset must already be in tensorflow format. The records will be written with\n",
      " |      keys from `dataset._format_columns`.\n",
      " |      \n",
      " |      Args:\n",
      " |          filename (:obj:`str`): The filename, including the `.tfrecord` extension, to write to.\n",
      " |          format (`str`, optional, default `\"tfrecord\"`): The type of output file. Currently this is a no-op, as\n",
      " |              TFRecords are the only option. This enables a more flexible function signature later.\n",
      " |  \n",
      " |  filter(self, function: Optional[Callable] = None, with_indices=False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, keep_in_memory: bool = False, load_from_cache_file: bool = True, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Optional[str] = None, desc: Optional[str] = None) -> 'Dataset'\n",
      " |      Apply a filter function to all the elements in the table in batches\n",
      " |      and update the table so that the dataset only includes examples according to the filter function.\n",
      " |      \n",
      " |      Args:\n",
      " |          function (:obj:`Callable`): Callable with one of the following signatures:\n",
      " |      \n",
      " |              - ``function(example: Dict[str, Any]) -> bool`` if ``with_indices=False, batched=False``\n",
      " |              - ``function(example: Dict[str, Any], indices: int) -> bool`` if ``with_indices=True, batched=False``\n",
      " |              - ``function(example: Dict[str, List]) -> List[bool]`` if ``with_indices=False, batched=True``\n",
      " |              - ``function(example: Dict[str, List], indices: List[int]) -> List[bool]`` if ``with_indices=True, batched=True``\n",
      " |      \n",
      " |              If no function is provided, defaults to an always True function: ``lambda x: True``.\n",
      " |          with_indices (:obj:`bool`, default `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      " |          input_columns (:obj:`str` or `List[str]`, optional): The columns to be passed into `function` as\n",
      " |              positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batched (:obj:`bool`, defaults to `False`): Provide batch of examples to `function`\n",
      " |          batch_size (:obj:`int`, optional, default `1000`): Number of examples per batch provided to `function` if\n",
      " |              ``batched = True``. If ``batched = False``, one example per batch is passed to ``function``.\n",
      " |              If ``batch_size <= 0`` or ``batch_size == None``: provide the full dataset as a single batch to `function`\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          fn_kwargs (:obj:`dict`, optional): Keyword arguments to be passed to `function`\n",
      " |          num_proc (:obj:`int`, optional): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing.\n",
      " |          suffix_template (:obj:`str`):\n",
      " |              If `cache_file_name` is specified, then this suffix will be added at the end of the base name of each.\n",
      " |              For example, if `cache_file_name` is `\"processed.arrow\"`, then for ``rank = 1`` and ``num_proc = 4``,\n",
      " |              the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix (default\n",
      " |              `_{rank:05d}_of_{num_proc:05d}`)\n",
      " |          new_fingerprint (:obj:`str`, optional): The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |          desc (:obj:`str`, optional, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.filter(lambda x: x[\"label\"] == 1)\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 533\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  flatten(self, new_fingerprint: Optional[str] = None, max_depth=16) -> 'Dataset'\n",
      " |      Flatten the table.\n",
      " |      Each column with a struct type is flattened into one column per struct field.\n",
      " |      Other columns are left unchanged.\n",
      " |      \n",
      " |      Args:\n",
      " |          new_fingerprint (:obj:`str`, optional): The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`: A copy of the dataset with flattened columns.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"squad\", split=\"train\")\n",
      " |      >>> ds.features\n",
      " |      {'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
      " |       'context': Value(dtype='string', id=None),\n",
      " |       'id': Value(dtype='string', id=None),\n",
      " |       'question': Value(dtype='string', id=None),\n",
      " |       'title': Value(dtype='string', id=None)}\n",
      " |      >>> ds.flatten()\n",
      " |      Dataset({\n",
      " |          features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
      " |          num_rows: 87599\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  flatten_indices(self, keep_in_memory: bool = False, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, features: Optional[datasets.features.features.Features] = None, disable_nullable: bool = False, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Create and cache a new Dataset by flattening the indices mapping.\n",
      " |      \n",
      " |      Args:\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          cache_file_name (:obj:`str`, optional, default `None`): Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          features (`Optional[datasets.Features]`, default `None`): Use a specific Features to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (:obj:`bool`, default `False`): Allow null values in the table.\n",
      " |          new_fingerprint (:obj:`str`, optional, default `None`): The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |  \n",
      " |  formatted_as(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      To be used in a ``with`` statement. Set ``__getitem__`` return format (type and columns).\n",
      " |      \n",
      " |      Args:\n",
      " |          type (:obj:`str`, optional): output type selected in ``[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow']``\n",
      " |              None means ``__getitem__`` returns python objects (default)\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
      " |              None means ``__getitem__`` returns all columns (default)\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |  \n",
      " |  map(self, function: Optional[Callable] = None, with_indices: bool = False, with_rank: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, drop_last_batch: bool = False, remove_columns: Union[str, List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: bool = None, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, features: Optional[datasets.features.features.Features] = None, disable_nullable: bool = False, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Optional[str] = None, desc: Optional[str] = None) -> 'Dataset'\n",
      " |      Apply a function to all the examples in the table (individually or in batches) and update the table.\n",
      " |      If your function returns a column that already exists, then it overwrites it.\n",
      " |      \n",
      " |      You can specify whether the function should be batched or not with the ``batched`` parameter:\n",
      " |      \n",
      " |      - If batched is False, then the function takes 1 example in and should return 1 example.\n",
      " |        An example is a dictionary, e.g. {\"text\": \"Hello there !\"}\n",
      " |      - If batched is True and batch_size is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\n",
      " |        A batch is a dictionary, e.g. a batch of 1 example is {\"text\": [\"Hello there !\"]}\n",
      " |      - If batched is True and batch_size is ``n`` > 1, then the function takes a batch of ``n`` examples as input and can return a batch with ``n`` examples, or with an arbitrary number of examples.\n",
      " |        Note that the last batch may have less than ``n`` examples.\n",
      " |        A batch is a dictionary, e.g. a batch of ``n`` examples is {\"text\": [\"Hello there !\"] * n}\n",
      " |      \n",
      " |      Args:\n",
      " |          function (:obj:`Callable`): Function with one of the following signatures:\n",
      " |      \n",
      " |              - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False` and `with_rank=False`\n",
      " |              - `function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      " |              - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False` and `with_rank=False`\n",
      " |              - `function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
      " |      \n",
      " |              For advanced usage, the function can also return a `pyarrow.Table`.\n",
      " |              Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n",
      " |              If no function is provided, default to identity function: ``lambda x: x``.\n",
      " |          with_indices (:obj:`bool`, default `False`): Provide example indices to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example, idx[, rank]): ...`.\n",
      " |          with_rank (:obj:`bool`, default `False`): Provide process rank to `function`. Note that in this case the\n",
      " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
      " |          input_columns (`Optional[Union[str, List[str]]]`, default `None`): The columns to be passed into `function`\n",
      " |              as positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      " |          batched (:obj:`bool`, default `False`): Provide batch of examples to `function`.\n",
      " |          batch_size (:obj:`int`, optional, default `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      " |              `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`.\n",
      " |          drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n",
      " |              dropped instead of being processed by the function.\n",
      " |          remove_columns (`Optional[Union[str, List[str]]]`, default `None`): Remove a selection of columns while doing the mapping.\n",
      " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      " |              columns with names in `remove_columns`, these columns will be kept.\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (:obj:`bool`, default `True` if caching is enabled): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          cache_file_name (:obj:`str`, optional, default `None`): Provide the name of a path for the cache file. It is used to store the\n",
      " |              results of the computation instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          features (`Optional[datasets.Features]`, default `None`): Use a specific Features to store the cache file\n",
      " |              instead of the automatically generated one.\n",
      " |          disable_nullable (:obj:`bool`, default `False`): Disallow null values in the table.\n",
      " |          fn_kwargs (:obj:`Dict`, optional, default `None`): Keyword arguments to be passed to `function`.\n",
      " |          num_proc (:obj:`int`, optional, default `None`): Max number of processes when generating cache. Already cached shards are loaded sequentially\n",
      " |          suffix_template (:obj:`str`):\n",
      " |              If cache_file_name is specified, then this suffix\n",
      " |              will be added at the end of the base name of each: defaults to \"_{rank:05d}_of_{num_proc:05d}\". For example, if cache_file_name is \"processed.arrow\", then for\n",
      " |              rank=1 and num_proc=4, the resulting file would be \"processed_00001_of_00004.arrow\" for the default suffix.\n",
      " |          new_fingerprint (:obj:`str`, optional, default `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |          desc (:obj:`str`, optional, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> def add_prefix(example):\n",
      " |      ...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      " |      ...     return example\n",
      " |      >>> ds = ds.map(add_prefix)\n",
      " |      >>> ds[0:3][\"text\"]\n",
      " |      ['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n",
      " |       'Review: the soundtrack alone is worth the price of admission .',\n",
      " |       'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\n",
      " |      \n",
      " |      # process a batch of examples\n",
      " |      >>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
      " |      # set number of processors\n",
      " |      >>> ds = ds.map(add_prefix, num_proc=4)\n",
      " |      ```\n",
      " |  \n",
      " |  prepare_for_task(self, task: Union[str, datasets.tasks.base.TaskTemplate], id: int = 0) -> 'Dataset'\n",
      " |      Prepare a dataset for the given task by casting the dataset's [`Features`] to standardized column names and types as detailed in [`datasets.tasks`](./package_reference/task_templates).\n",
      " |      \n",
      " |      Casts [`datasets.DatasetInfo.features`] according to a task-specific schema. Intended for single-use only, so all task templates are removed from [`datasets.DatasetInfo.task_templates`] after casting.\n",
      " |      \n",
      " |      Args:\n",
      " |          task (`Union[str, TaskTemplate]`): The task to prepare the dataset for during training and evaluation. If `str`, supported tasks include:\n",
      " |      \n",
      " |              - `\"text-classification\"`\n",
      " |              - `\"question-answering\"`\n",
      " |      \n",
      " |              If [`TaskTemplate`], must be one of the task templates in [`datasets.tasks`](./package_reference/task_templates).\n",
      " |          id (`int`, defaults to 0): The id required to unambiguously identify the task template when multiple task templates of the same type are supported.\n",
      " |  \n",
      " |  push_to_hub(self, repo_id: str, split: Optional[str] = None, private: Optional[bool] = False, token: Optional[str] = None, branch: Optional[str] = None, max_shard_size: Union[int, str, NoneType] = None, shard_size: Optional[int] = 'deprecated', embed_external_files: bool = True)\n",
      " |      Pushes the dataset to the hub as a Parquet dataset.\n",
      " |      The dataset is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
      " |      \n",
      " |      The resulting Parquet files are self-contained by default: if your dataset contains :class:`Image` or :class:`Audio`\n",
      " |      data, the Parquet files will store the bytes of your images or audio files.\n",
      " |      You can disable this by setting `embed_external_files` to False.\n",
      " |      \n",
      " |      Args:\n",
      " |          repo_id (:obj:`str`):\n",
      " |              The ID of the repository to push to in the following format: `<user>/<dataset_name>` or\n",
      " |              `<org>/<dataset_name>`. Also accepts `<dataset_name>`, which will default to the namespace\n",
      " |              of the logged-in user.\n",
      " |          split (Optional, :obj:`str`):\n",
      " |              The name of the split that will be given to that dataset. Defaults to `self.split`.\n",
      " |          private (Optional :obj:`bool`, defaults to :obj:`False`):\n",
      " |              Whether the dataset repository should be set to private or not. Only affects repository creation:\n",
      " |              a repository that already exists will not be affected by that parameter.\n",
      " |          token (Optional :obj:`str`):\n",
      " |              An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n",
      " |              to the token saved locally when logging in with ``huggingface-cli login``. Will raise an error\n",
      " |              if no token is passed and the user is not logged-in.\n",
      " |          branch (Optional :obj:`str`):\n",
      " |              The git branch on which to push the dataset. This defaults to the default branch as specified\n",
      " |              in your repository, which defaults to `\"main\"`.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
      " |              The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n",
      " |              (like `\"5MB\"`).\n",
      " |          shard_size (Optional :obj:`int`):\n",
      " |              Deprecated: 'shard_size' was renamed to 'max_shard_size' in version 2.1.1 and will be removed in 2.4.0.\n",
      " |          embed_external_files (:obj:`bool`, default ``True``):\n",
      " |              Whether to embed file bytes in the shards.\n",
      " |              In particular, this will do the following before the push for the fields of type:\n",
      " |      \n",
      " |              - :class:`Audio` and class:`Image`: remove local path information and embed file content in the Parquet files.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"evaluation\")\n",
      " |      ```\n",
      " |  \n",
      " |  remove_columns(self, column_names: Union[str, List[str]], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Remove one or several column(s) in the dataset and the features associated to them.\n",
      " |      \n",
      " |      You can also remove a column using :func:`Dataset.map` with `remove_columns` but the present method\n",
      " |      is in-place (doesn't copy the data to a new dataset) and is thus faster.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_names (:obj:`Union[str, List[str]]`): Name of the column(s) to remove.\n",
      " |          new_fingerprint (:obj:`str`, optional): The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`: A copy of the dataset object without the columns to remove.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.remove_columns('label')\n",
      " |      Dataset({\n",
      " |          features: ['text'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  rename_column(self, original_column_name: str, new_column_name: str, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Rename a column in the dataset, and move the features associated to the original column under the new column\n",
      " |      name.\n",
      " |      \n",
      " |      Args:\n",
      " |          original_column_name (:obj:`str`): Name of the column to rename.\n",
      " |          new_column_name (:obj:`str`): New name for the column.\n",
      " |          new_fingerprint (:obj:`str`, optional): The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`: A copy of the dataset with a renamed column.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.rename_column('label', 'label_new')\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label_new'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  rename_columns(self, column_mapping: Dict[str, str], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Rename several columns in the dataset, and move the features associated to the original columns under\n",
      " |      the new column names.\n",
      " |      \n",
      " |      Args:\n",
      " |          column_mapping (:obj:`Dict[str, str]`): A mapping of columns to rename to their new names\n",
      " |          new_fingerprint (:obj:`str`, optional): The new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`: A copy of the dataset with renamed columns\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n",
      " |      Dataset({\n",
      " |          features: ['text_new', 'label_new'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  reset_format(self)\n",
      " |      Reset __getitem__ return format to python objects and all columns.\n",
      " |      \n",
      " |      Same as ``self.set_format()``\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'numpy'}\n",
      " |      >>> ds.reset_format()\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': None}\n",
      " |      ```\n",
      " |  \n",
      " |  save_to_disk(self, dataset_path: str, fs=None)\n",
      " |      Saves a dataset to a dataset directory, or in a filesystem using either :class:`~filesystems.S3FileSystem` or\n",
      " |      any implementation of ``fsspec.spec.AbstractFileSystem``.\n",
      " |      \n",
      " |      For :class:`Image` and :class:`Audio` data:\n",
      " |      \n",
      " |      If your images and audio files are local files, then the resulting arrow file will store paths to these files.\n",
      " |      If you want to include the bytes or your images or audio files instead, you must `read()` those files first.\n",
      " |      This can be done by storing the \"bytes\" instead of the \"path\" of the images or audio files:\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> def read_image_file(example):\n",
      " |      ...     with open(example[\"image\"].filename, \"rb\") as f:\n",
      " |      ...         return {\"image\": {\"bytes\": f.read()}}\n",
      " |      >>> ds = ds.map(read_image_file)\n",
      " |      >>> ds.save_to_disk(\"path/to/dataset/dir\")\n",
      " |      ```\n",
      " |      \n",
      " |      ```python\n",
      " |      >>> def read_audio_file(example):\n",
      " |      ...     with open(example[\"audio\"][\"path\"], \"rb\") as f:\n",
      " |      ...         return {\"audio\": {\"bytes\": f.read()}}\n",
      " |      >>> ds = ds.map(read_audio_file)\n",
      " |      >>> ds.save_to_disk(\"path/to/dataset/dir\")\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_path (:obj:`str`): Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n",
      " |              of the dataset directory where the dataset will be saved to.\n",
      " |          fs (:class:`~filesystems.S3FileSystem`, ``fsspec.spec.AbstractFileSystem``, optional, defaults ``None``):\n",
      " |              Instance of the remote filesystem used to download the files from.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> saved_ds = ds.save_to_disk(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  select(self, indices: Iterable, keep_in_memory: bool = False, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Create a new dataset with rows selected following the list/array of indices.\n",
      " |      \n",
      " |      Args:\n",
      " |          indices (range, list, iterable, ndarray or Series): Range, list or 1D-array of integer indices for indexing.\n",
      " |              If the indices correspond to a contiguous range, the Arrow table is simply sliced.\n",
      " |              However passing a list of indices that are not contiguous creates indices mapping, which is much less efficient,\n",
      " |              but still faster than recreating an Arrow table made of the requested rows.\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the indices mapping in memory instead of writing it to a cache file.\n",
      " |          indices_cache_file_name (:obj:`str`, optional, default `None`): Provide the name of a path for the cache file. It is used to store the\n",
      " |              indices mapping instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          new_fingerprint (:obj:`str`, optional, default `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.select(range(4))\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 4\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  set_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      Set __getitem__ return format (type and columns). The data formatting is applied on-the-fly.\n",
      " |      The format ``type`` (for example \"numpy\") is used to format batches when using __getitem__.\n",
      " |      It's also possible to use custom transforms for formatting using :func:`datasets.Dataset.set_transform`.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (:obj:`str`, optional):\n",
      " |              Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow'].\n",
      " |              None means __getitem__ returns python objects (default)\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output.\n",
      " |              None means __getitem__ returns all columns (default).\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |      \n",
      " |      It is possible to call ``map`` after calling ``set_format``. Since ``map`` may add new columns, then the list of formatted columns\n",
      " |      gets updated. In this case, if you apply ``map`` on a dataset to add a new column, then this column will be formatted:\n",
      " |      \n",
      " |          new formatted columns = (all columns - previously unformatted columns)\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.set_format(type='numpy', columns=['text', 'label'])\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'numpy'}\n",
      " |      ```\n",
      " |  \n",
      " |  set_transform(self, transform: Optional[Callable], columns: Optional[List] = None, output_all_columns: bool = False)\n",
      " |      Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called.\n",
      " |      As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`\n",
      " |      \n",
      " |      Args:\n",
      " |          transform (:obj:`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n",
      " |              A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n",
      " |              This function is applied right before returning the objects in __getitem__.\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
      " |              If specified, then the input batch of the transform only contains those columns.\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
      " |      >>> def encode(batch):\n",
      " |      ...     return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n",
      " |      >>> ds.set_transform(encode)\n",
      " |      >>> ds[0]\n",
      " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |       1, 1]),\n",
      " |       'input_ids': tensor([  101, 29353,  2135, 15102,  1996,  9428, 20868,  2890,  8663,  6895,\n",
      " |               20470,  2571,  3663,  2090,  4603,  3017,  3008,  1998,  2037, 24211,\n",
      " |               5637,  1998, 11690,  2336,  1012,   102]),\n",
      " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |               0, 0])}\n",
      " |      ```\n",
      " |  \n",
      " |  shard(self, num_shards: int, index: int, contiguous: bool = False, keep_in_memory: bool = False, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000) -> 'Dataset'\n",
      " |      Return the `index`-nth shard from dataset split into `num_shards` pieces.\n",
      " |      \n",
      " |      This shards deterministically. dset.shard(n, i) will contain all elements of dset whose\n",
      " |      index mod n = i.\n",
      " |      \n",
      " |      dset.shard(n, i, contiguous=True) will instead split dset into contiguous chunks,\n",
      " |      so it can be easily concatenated back together after processing. If n % i == l, then the\n",
      " |      first l shards will have length (n // i) + 1, and the remaining shards will have length (n // i).\n",
      " |      `datasets.concatenate([dset.shard(n, i, contiguous=True) for i in range(n)])` will return\n",
      " |      a dataset with the same order as the original.\n",
      " |      \n",
      " |      Be sure to shard before using any randomizing operator (such as shuffle).\n",
      " |      It is best if the shard operator is used early in the dataset pipeline.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          num_shards (:obj:`int`): How many shards to split the dataset into.\n",
      " |          index (:obj:`int`): Which shard to select and return.\n",
      " |          contiguous: (:obj:`bool`, default `False`): Whether to select contiguous blocks of indices for shards.\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the current computation from `function`\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n",
      " |              indices of each shard instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 1066\n",
      " |      })\n",
      " |      >>> ds.shard(num_shards=2, index=0)\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 533\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  shuffle(self, seed: Optional[int] = None, generator: Optional[numpy.random._generator.Generator] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Create a new Dataset where the rows are shuffled.\n",
      " |      \n",
      " |      Currently shuffling uses numpy random generators.\n",
      " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
      " |      \n",
      " |      Args:\n",
      " |          seed (:obj:`int`, optional): A seed to initialize the default BitGenerator if ``generator=None``.\n",
      " |              If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |          generator (:obj:`numpy.random.Generator`, optional): Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the shuffled indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the shuffled indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n",
      " |              shuffled indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          new_fingerprint (:obj:`str`, optional, default `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds['label'][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      \n",
      " |      # set a seed\n",
      " |      >>> shuffled_ds = ds.shuffle(seed=42)\n",
      " |      >>> shuffled_ds['label'][:10]\n",
      " |      [1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      " |      ```\n",
      " |  \n",
      " |  sort(self, column: str, reverse: bool = False, kind: str = None, null_placement: str = 'last', keep_in_memory: bool = False, load_from_cache_file: bool = True, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
      " |      Create a new dataset sorted according to a column.\n",
      " |      \n",
      " |      Currently sorting according to a column name uses pandas sorting algorithm under the hood.\n",
      " |      The column should thus be a pandas compatible type (in particular not a nested type).\n",
      " |      This also means that the column used for sorting is fully loaded in memory (which should be fine in most cases).\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`): column name to sort by.\n",
      " |          reverse (:obj:`bool`, default `False`): If True, sort by descending order rather then ascending.\n",
      " |          kind (:obj:`str`, optional): Pandas algorithm for sorting selected in {‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’},\n",
      " |              The default is ‘quicksort’. Note that both ‘stable’ and ‘mergesort’ use timsort under the covers and, in general,\n",
      " |              the actual implementation will vary with data type. The ‘mergesort’ option is retained for backwards compatibility.\n",
      " |          null_placement (:obj:`str`, default `last`):\n",
      " |              Put `None` values at the beginning if ‘first‘; ‘last‘ puts `None` values at the end.\n",
      " |      \n",
      " |              *New in version 1.14.2*\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the sorted indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the sorted indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          indices_cache_file_name (:obj:`str`, optional, default `None`): Provide the name of a path for the cache file. It is used to store the\n",
      " |              sorted indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              Higher value gives smaller cache files, lower value consume less temporary memory.\n",
      " |          new_fingerprint (:obj:`str`, optional, default `None`): the new fingerprint of the dataset after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds['label'][:10]\n",
      " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      " |      >>> sorted_ds = ds.sort('label')\n",
      " |      >>> sorted_ds['label'][:10]\n",
      " |      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " |      ```\n",
      " |  \n",
      " |  to_csv(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, num_proc: Optional[int] = None, **to_csv_kwargs) -> int\n",
      " |      Exports the dataset to csv\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_buf (``PathLike`` or ``FileOrBuffer``): Either a path to a file or a BinaryIO.\n",
      " |          batch_size (:obj:`int`, optional): Size of the batch to load in memory and write at once.\n",
      " |              Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |          num_proc (:obj:`int`, optional): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing. ``batch_size`` in this case defaults to\n",
      " |              :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n",
      " |              value if you have sufficient compute power.\n",
      " |          **to_csv_kwargs (additional keyword arguments): Parameters to pass to pandas's :func:`pandas.DataFrame.to_csv`\n",
      " |      \n",
      " |      Returns:\n",
      " |          int: The number of characters or bytes written\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_csv(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  to_dict(self, batch_size: Optional[int] = None, batched: bool = False) -> Union[dict, Iterator[dict]]\n",
      " |      Returns the dataset as a Python dict. Can also return a generator for large datasets.\n",
      " |      \n",
      " |      Args:\n",
      " |          batched (:obj:`bool`): Set to :obj:`True` to return a generator that yields the dataset as batches\n",
      " |              of ``batch_size`` rows. Defaults to :obj:`False` (returns the whole datasets once)\n",
      " |          batch_size (:obj:`int`, optional): The size (number of rows) of the batches if ``batched`` is `True`.\n",
      " |              Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `dict` or `Iterator[dict]`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_dict()\n",
      " |      ```\n",
      " |  \n",
      " |  to_json(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, num_proc: Optional[int] = None, **to_json_kwargs) -> int\n",
      " |      Export the dataset to JSON Lines or JSON.\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_buf (``PathLike`` or ``FileOrBuffer``): Either a path to a file or a BinaryIO.\n",
      " |          batch_size (:obj:`int`, optional): Size of the batch to load in memory and write at once.\n",
      " |              Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |          num_proc (:obj:`int`, optional): Number of processes for multiprocessing. By default it doesn't\n",
      " |              use multiprocessing. ``batch_size`` in this case defaults to\n",
      " |              :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n",
      " |              value if you have sufficient compute power.\n",
      " |          lines (:obj:`bool`, default ``True``): Whether output JSON lines format.\n",
      " |              Only possible if ``orient=\"records\"`. It will throw ValueError with ``orient`` different from\n",
      " |              ``\"records\"``, since the others are not list-like.\n",
      " |          orient (:obj:`str`, default ``\"records\"``): Format of the JSON:\n",
      " |      \n",
      " |              - ``\"records\"``: list like ``[{column -> value}, … , {column -> value}]``\n",
      " |              - ``\"split\"``: dict like ``{\"index\" -> [index], \"columns\" -> [columns], \"data\" -> [values]}``\n",
      " |              - ``\"index\"``: dict like ``{index -> {column -> value}}``\n",
      " |              - ``\"columns\"``: dict like ``{column -> {index -> value}}``\n",
      " |              - ``\"values\"``: just the values array\n",
      " |              - ``\"table\"``: dict like ``{\"schema\": {schema}, \"data\": {data}}``\n",
      " |          **to_json_kwargs (additional keyword arguments): Parameters to pass to pandas's `pandas.DataFrame.to_json\n",
      " |              <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html>`_.\n",
      " |      \n",
      " |      Returns:\n",
      " |          int: The number of characters or bytes written.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_json(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  to_pandas(self, batch_size: Optional[int] = None, batched: bool = False) -> Union[pandas.core.frame.DataFrame, Iterator[pandas.core.frame.DataFrame]]\n",
      " |      Returns the dataset as a :class:`pandas.DataFrame`. Can also return a generator for large datasets.\n",
      " |      \n",
      " |      Args:\n",
      " |          batched (:obj:`bool`): Set to :obj:`True` to return a generator that yields the dataset as batches\n",
      " |              of ``batch_size`` rows. Defaults to :obj:`False` (returns the whole datasets once)\n",
      " |          batch_size (:obj:`int`, optional): The size (number of rows) of the batches if ``batched`` is `True`.\n",
      " |              Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `pandas.DataFrame` or `Iterator[pandas.DataFrame]`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_pandas()\n",
      " |      ```\n",
      " |  \n",
      " |  to_parquet(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, **parquet_writer_kwargs) -> int\n",
      " |      Exports the dataset to parquet\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_buf (``PathLike`` or ``FileOrBuffer``): Either a path to a file or a BinaryIO.\n",
      " |          batch_size (:obj:`int`, optional): Size of the batch to load in memory and write at once.\n",
      " |              Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |          **parquet_writer_kwargs (additional keyword arguments): Parameters to pass to PyArrow's :class:`pyarrow.parquet.ParquetWriter`\n",
      " |      \n",
      " |      Returns:\n",
      " |          int: The number of characters or bytes written\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds.to_parquet(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  to_sql(self, name: str, con: Union[str, ForwardRef('sqlalchemy.engine.Connection'), ForwardRef('sqlalchemy.engine.Engine'), ForwardRef('sqlite3.Connection')], batch_size: Optional[int] = None, **sql_writer_kwargs) -> int\n",
      " |      Exports the dataset to a SQL database.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (`str`): Name of SQL table.\n",
      " |          con (`str` or :obj:`sqlite3.Connection` or :obj:`sqlalchemy.engine.Connection` or :obj:`sqlalchemy.engine.Connection`):\n",
      " |              A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) or a SQLite3/SQLAlchemy connection object used to write to a database.\n",
      " |          batch_size (:obj:`int`, optional): Size of the batch to load in memory and write at once.\n",
      " |              Defaults to :obj:`datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
      " |          **sql_writer_kwargs (additional keyword arguments): Parameters to pass to pandas's :function:`Dataframe.to_sql`\n",
      " |      \n",
      " |      Returns:\n",
      " |          int: The number of records written.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> # con provided as a connection URI string\n",
      " |      >>> ds.to_sql(\"data\", \"sqlite:///my_own_db.sql\")\n",
      " |      >>> # con provided as a sqlite3 connection object\n",
      " |      >>> import sqlite3\n",
      " |      >>> con = sqlite3.connect(\"my_own_db.sql\")\n",
      " |      >>> with con:\n",
      " |      ...     ds.to_sql(\"data\", con)\n",
      " |      ```\n",
      " |  \n",
      " |  train_test_split(self, test_size: Union[float, int, NoneType] = None, train_size: Union[float, int, NoneType] = None, shuffle: bool = True, stratify_by_column: Optional[str] = None, seed: Optional[int] = None, generator: Optional[numpy.random._generator.Generator] = None, keep_in_memory: bool = False, load_from_cache_file: bool = True, train_indices_cache_file_name: Optional[str] = None, test_indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, train_new_fingerprint: Optional[str] = None, test_new_fingerprint: Optional[str] = None) -> 'DatasetDict'\n",
      " |      Return a dictionary (:obj:`datasets.DatasetDict`) with two random train and test subsets (`train` and `test` ``Dataset`` splits).\n",
      " |      Splits are created from the dataset according to `test_size`, `train_size` and `shuffle`.\n",
      " |      \n",
      " |      This method is similar to scikit-learn `train_test_split`.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_size (:obj:`numpy.random.Generator`, optional): Size of the test split\n",
      " |              If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split.\n",
      " |              If int, represents the absolute number of test samples.\n",
      " |              If None, the value is set to the complement of the train size.\n",
      " |              If train_size is also None, it will be set to 0.25.\n",
      " |          train_size (:obj:`numpy.random.Generator`, optional): Size of the train split\n",
      " |              If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |              If int, represents the absolute number of train samples.\n",
      " |              If None, the value is automatically set to the complement of the test size.\n",
      " |          shuffle (:obj:`bool`, optional, default `True`): Whether or not to shuffle the data before splitting.\n",
      " |          stratify_by_column (:obj:`str`, optional, default `None`): The column name of labels to be used to perform stratified split of data.\n",
      " |          seed (:obj:`int`, optional): A seed to initialize the default BitGenerator if ``generator=None``.\n",
      " |              If None, then fresh, unpredictable entropy will be pulled from the OS.\n",
      " |              If an int or array_like[ints] is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
      " |          generator (:obj:`numpy.random.Generator`, optional): Numpy random Generator to use to compute the permutation of the dataset rows.\n",
      " |              If ``generator=None`` (default), uses np.random.default_rng (the default BitGenerator (PCG64) of NumPy).\n",
      " |          keep_in_memory (:obj:`bool`, default `False`): Keep the splits indices in memory instead of writing it to a cache file.\n",
      " |          load_from_cache_file (:obj:`bool`, default `True`): If a cache file storing the splits indices\n",
      " |              can be identified, use it instead of recomputing.\n",
      " |          train_cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n",
      " |              train split indices instead of the automatically generated cache file name.\n",
      " |          test_cache_file_name (:obj:`str`, optional): Provide the name of a path for the cache file. It is used to store the\n",
      " |              test split indices instead of the automatically generated cache file name.\n",
      " |          writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      " |          train_new_fingerprint (:obj:`str`, optional, defaults to `None`): the new fingerprint of the train set after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |          test_new_fingerprint (:obj:`str`, optional, defaults to `None`): the new fingerprint of the test set after transform.\n",
      " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds = ds.train_test_split(test_size=0.2, shuffle=True)\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 852\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 214\n",
      " |          })\n",
      " |      })\n",
      " |      \n",
      " |      # set a seed\n",
      " |      >>> ds = ds.train_test_split(test_size=0.2, seed=42)\n",
      " |      \n",
      " |      # stratified split\n",
      " |      >>> ds = load_dataset(\"imdb\",split=\"train\")\n",
      " |      Dataset({\n",
      " |          features: ['text', 'label'],\n",
      " |          num_rows: 25000\n",
      " |      })\n",
      " |      >>> ds = ds.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
      " |      DatasetDict({\n",
      " |          train: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 20000\n",
      " |          })\n",
      " |          test: Dataset({\n",
      " |              features: ['text', 'label'],\n",
      " |              num_rows: 5000\n",
      " |          })\n",
      " |      })\n",
      " |      ```\n",
      " |  \n",
      " |  unique(self, column: str) -> List\n",
      " |      Return a list of the unique elements in a column.\n",
      " |      \n",
      " |      This is implemented in the low-level backend and as such, very fast.\n",
      " |      \n",
      " |      Args:\n",
      " |          column (:obj:`str`): Column name (list all the column names with :func:`datasets.Dataset.column_names`).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :obj:`list`: List of unique elements in the given column.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.unique('label')\n",
      " |      [1, 0]\n",
      " |      ```\n",
      " |  \n",
      " |  with_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
      " |      Set __getitem__ return format (type and columns). The data formatting is applied on-the-fly.\n",
      " |      The format ``type`` (for example \"numpy\") is used to format batches when using __getitem__.\n",
      " |      \n",
      " |      It's also possible to use custom transforms for formatting using :func:`datasets.Dataset.with_transform`.\n",
      " |      \n",
      " |      Contrary to :func:`datasets.Dataset.set_format`, ``with_format`` returns a new Dataset object.\n",
      " |      \n",
      " |      Args:\n",
      " |          type (:obj:`str`, optional):\n",
      " |              Either output type selected in [None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow'].\n",
      " |              None means __getitem__ returns python objects (default)\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
      " |              None means __getitem__ returns all columns (default)\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |          **format_kwargs (additional keyword arguments): keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': None}\n",
      " |      >>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
      " |      >>> ds.format\n",
      " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |       'format_kwargs': {},\n",
      " |       'output_all_columns': False,\n",
      " |       'type': 'tensorflow'}\n",
      " |      ```\n",
      " |  \n",
      " |  with_transform(self, transform: Optional[Callable], columns: Optional[List] = None, output_all_columns: bool = False)\n",
      " |      Set __getitem__ return format using this transform. The transform is applied on-the-fly on batches when __getitem__ is called.\n",
      " |      \n",
      " |      As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`.\n",
      " |      \n",
      " |      Contrary to :func:`datasets.Dataset.set_transform`, ``with_transform`` returns a new Dataset object.\n",
      " |      \n",
      " |      Args:\n",
      " |          transform (:obj:`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`\n",
      " |              A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.\n",
      " |              This function is applied right before returning the objects in __getitem__.\n",
      " |          columns (:obj:`List[str]`, optional): columns to format in the output\n",
      " |              If specified, then the input batch of the transform only contains those columns.\n",
      " |          output_all_columns (:obj:`bool`, default to False): keep un-formatted columns as well in the output (as python objects)\n",
      " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> from transformers import AutoTokenizer\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      " |      >>> def encode(example):\n",
      " |      ...     return tokenizer(example[\"text\"], padding=True, truncation=True, return_tensors='pt')\n",
      " |      >>> ds = ds.with_transform(encode)\n",
      " |      >>> ds[0]\n",
      " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      " |       1, 1, 1, 1, 1]),\n",
      " |       'input_ids': tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,\n",
      " |               1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,\n",
      " |               1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102]),\n",
      " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      " |               0, 0, 0, 0, 0])}\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  from_buffer(buffer: pyarrow.lib.Buffer, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_buffer: Optional[pyarrow.lib.Buffer] = None) -> 'Dataset' from builtins.type\n",
      " |      Instantiate a Dataset backed by an Arrow buffer.\n",
      " |      \n",
      " |      Args:\n",
      " |          buffer (:obj:`pyarrow.Buffer`): Arrow buffer.\n",
      " |          info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
      " |          split (:class:`NamedSplit`, optional): Name of the dataset split.\n",
      " |          indices_buffer (:obj:`pyarrow.Buffer`, optional): Indices Arrow buffer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |  \n",
      " |  from_dict(mapping: dict, features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None) -> 'Dataset' from builtins.type\n",
      " |      Convert :obj:`dict` to a :obj:`pyarrow.Table` to create a :class:`Dataset`.\n",
      " |      \n",
      " |      Args:\n",
      " |          mapping (:obj:`Mapping`): Mapping of strings to Arrays or Python lists.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
      " |          split (:class:`NamedSplit`, optional): Name of the dataset split.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |  \n",
      " |  from_file(filename: str, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_filename: Optional[str] = None, in_memory: bool = False) -> 'Dataset' from builtins.type\n",
      " |      Instantiate a Dataset backed by an Arrow table at filename.\n",
      " |      \n",
      " |      Args:\n",
      " |          filename (:obj:`str`): File name of the dataset.\n",
      " |          info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
      " |          split (:class:`NamedSplit`, optional): Name of the dataset split.\n",
      " |          indices_filename (:obj:`str`, optional): File names of the indices.\n",
      " |          in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |  \n",
      " |  from_list(mapping: List[dict], features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None) -> 'Dataset' from builtins.type\n",
      " |      Convert a list of dicts to a :obj:`pyarrow.Table` to create a :class:`Dataset`.\n",
      " |      \n",
      " |      Note that the keys of the first entry will be used to determine the dataset columns,\n",
      " |      regardless of what is passed to features.\n",
      " |      \n",
      " |      Args:\n",
      " |          mapping (:obj:`List[dict]`): A list of mappings of strings to row values.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
      " |          split (:class:`NamedSplit`, optional): Name of the dataset split.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |  \n",
      " |  from_pandas(df: pandas.core.frame.DataFrame, features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, preserve_index: Optional[bool] = None) -> 'Dataset' from builtins.type\n",
      " |      Convert :obj:`pandas.DataFrame` to a :obj:`pyarrow.Table` to create a :class:`Dataset`.\n",
      " |      \n",
      " |      The column types in the resulting Arrow Table are inferred from the dtypes of the pandas.Series in the\n",
      " |      DataFrame. In the case of non-object Series, the NumPy dtype is translated to its Arrow equivalent. In the\n",
      " |      case of `object`, we need to guess the datatype by looking at the Python objects in this Series.\n",
      " |      \n",
      " |      Be aware that Series of the `object` dtype don't carry enough information to always lead to a meaningful Arrow\n",
      " |      type. In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only\n",
      " |      contains None/nan objects, the type is set to null. This behavior can be avoided by constructing explicit\n",
      " |      features and passing it to this function.\n",
      " |      \n",
      " |      Args:\n",
      " |          df (:obj:`pandas.DataFrame`): Dataframe that contains the dataset.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          info (:class:`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
      " |          split (:class:`NamedSplit`, optional): Name of the dataset split.\n",
      " |          preserve_index (:obj:`bool`, optional): Whether to store the index as an additional column in the resulting Dataset.\n",
      " |              The default of None will store the index as a column, except for RangeIndex which is stored as metadata only.\n",
      " |              Use preserve_index=True to force it to be stored as a column.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_pandas(df)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_csv(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs)\n",
      " |      Create Dataset from CSV file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (path-like or list of path-like): Path(s) of the CSV file(s).\n",
      " |          split (:class:`NamedSplit`, optional): Split name to be assigned to the dataset.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (:obj:`str`, optional, default ``\"~/.cache/huggingface/datasets\"``): Directory to cache data.\n",
      " |          keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :meth:`pandas.read_csv`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_csv('path/to/dataset.csv')\n",
      " |      ```\n",
      " |  \n",
      " |  from_generator(generator: Callable, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, gen_kwargs: Optional[dict] = None, **kwargs)\n",
      " |      Create a Dataset from a generator.\n",
      " |      \n",
      " |      Args:\n",
      " |          generator (:obj:`Callable`): A generator function that `yields` examples.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (:obj:`str`, optional, default ``\"~/.cache/huggingface/datasets\"``): Directory to cache data.\n",
      " |          keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n",
      " |          gen_kwargs(:obj:`dict`, optional): Keyword arguments to be passed to the `generator` callable.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`GeneratorConfig`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> def gen():\n",
      " |      ...     yield {\"text\": \"Good\", \"label\": 0}\n",
      " |      ...     yield {\"text\": \"Bad\", \"label\": 1}\n",
      " |      ...\n",
      " |      >>> ds = Dataset.from_generator(gen)\n",
      " |      ```\n",
      " |  \n",
      " |  from_json(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, field: Optional[str] = None, **kwargs)\n",
      " |      Create Dataset from JSON or JSON Lines file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (path-like or list of path-like): Path(s) of the JSON or JSON Lines file(s).\n",
      " |          split (:class:`NamedSplit`, optional): Split name to be assigned to the dataset.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (:obj:`str`, optional, default ``\"~/.cache/huggingface/datasets\"``): Directory to cache data.\n",
      " |          keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n",
      " |          field (:obj:`str`, optional): Field name of the JSON file where the dataset is contained in.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`JsonConfig`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_json('path/to/dataset.json')\n",
      " |      ```\n",
      " |  \n",
      " |  from_parquet(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, columns: Optional[List[str]] = None, **kwargs)\n",
      " |      Create Dataset from Parquet file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (path-like or list of path-like): Path(s) of the Parquet file(s).\n",
      " |          split (:class:`NamedSplit`, optional): Split name to be assigned to the dataset.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (:obj:`str`, optional, default ``\"~/.cache/huggingface/datasets\"``): Directory to cache data.\n",
      " |          keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n",
      " |          columns (:obj:`List[str]`, optional): If not None, only these columns will be read from the file.\n",
      " |              A column name may be a prefix of a nested field, e.g. 'a' will select\n",
      " |              'a.b', 'a.c', and 'a.d.e'.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`ParquetConfig`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_parquet('path/to/dataset.parquet')\n",
      " |      ```\n",
      " |  \n",
      " |  from_sql(sql: Union[str, ForwardRef('sqlalchemy.sql.Selectable')], con: Union[str, ForwardRef('sqlalchemy.engine.Connection'), ForwardRef('sqlalchemy.engine.Engine'), ForwardRef('sqlite3.Connection')], features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs)\n",
      " |      Create Dataset from SQL query or database table.\n",
      " |      \n",
      " |      Args:\n",
      " |          sql (`str` or :obj:`sqlalchemy.sql.Selectable`): SQL query to be executed or a table name.\n",
      " |          con (`str` or :obj:`sqlite3.Connection` or :obj:`sqlalchemy.engine.Connection` or :obj:`sqlalchemy.engine.Connection`):\n",
      " |              A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) used to instantiate a database connection or a SQLite3/SQLAlchemy connection object.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (:obj:`str`, optional, default ``\"~/.cache/huggingface/datasets\"``): Directory to cache data.\n",
      " |          keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`SqlConfig`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> # Fetch a database table\n",
      " |      >>> ds = Dataset.from_sql(\"test_data\", \"postgres:///db_name\")\n",
      " |      >>> # Execute a SQL query on the table\n",
      " |      >>> ds = Dataset.from_sql(\"SELECT sentence FROM test_data\", \"postgres:///db_name\")\n",
      " |      >>> # Use a Selectable object to specify the query\n",
      " |      >>> from sqlalchemy import select, text\n",
      " |      >>> stmt = select([text(\"sentence\")]).select_from(text(\"test_data\"))\n",
      " |      >>> ds = Dataset.from_sql(stmt, \"postgres:///db_name\")\n",
      " |      ```\n",
      " |      \n",
      " |      <Tip {warning=true}>\n",
      " |      The returned dataset can only be cached if `con` is specified as URI string.\n",
      " |      </Tip>\n",
      " |  \n",
      " |  from_text(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs)\n",
      " |      Create Dataset from text file(s).\n",
      " |      \n",
      " |      Args:\n",
      " |          path_or_paths (path-like or list of path-like): Path(s) of the text file(s).\n",
      " |          split (:class:`NamedSplit`, optional): Split name to be assigned to the dataset.\n",
      " |          features (:class:`Features`, optional): Dataset features.\n",
      " |          cache_dir (:obj:`str`, optional, default ``\"~/.cache/huggingface/datasets\"``): Directory to cache data.\n",
      " |          keep_in_memory (:obj:`bool`, default ``False``): Whether to copy the data in-memory.\n",
      " |          **kwargs (additional keyword arguments): Keyword arguments to be passed to :class:`TextConfig`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = Dataset.from_text('path/to/dataset.txt')\n",
      " |      ```\n",
      " |  \n",
      " |  load_from_disk(dataset_path: str, fs=None, keep_in_memory: Optional[bool] = None) -> 'Dataset'\n",
      " |      Loads a dataset that was previously saved using :meth:`save_to_disk` from a dataset directory, or from a\n",
      " |      filesystem using either :class:`~filesystems.S3FileSystem` or any implementation of\n",
      " |      ``fsspec.spec.AbstractFileSystem``.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataset_path (:obj:`str`): Path (e.g. `\"dataset/train\"`) or remote URI (e.g.\n",
      " |              `\"s3//my-bucket/dataset/train\"`) of the dataset directory where the dataset will be loaded from.\n",
      " |          fs (:class:`~filesystems.S3FileSystem`, ``fsspec.spec.AbstractFileSystem``, optional, default ``None``):\n",
      " |              Instance of the remote filesystem used to download the files from.\n",
      " |          keep_in_memory (:obj:`bool`, default ``None``): Whether to copy the dataset in-memory. If `None`, the\n",
      " |              dataset will not be copied in-memory unless explicitly enabled by setting\n",
      " |              `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n",
      " |              :ref:`load_dataset_enhancing_performance` section.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`Dataset` or :class:`DatasetDict`:\n",
      " |          - If `dataset_path` is a path of a dataset directory: the dataset requested.\n",
      " |          - If `dataset_path` is a path of a dataset dict directory: a ``datasets.DatasetDict`` with each split.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds = load_from_disk(\"path/to/dataset/directory\")\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  cache_files\n",
      " |      The cache files containing the Apache Arrow table backing the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.cache_files\n",
      " |      [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]\n",
      " |      ```\n",
      " |  \n",
      " |  column_names\n",
      " |      Names of the columns in the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.column_names\n",
      " |      ['text', 'label']\n",
      " |      ```\n",
      " |  \n",
      " |  data\n",
      " |      The Apache Arrow table backing the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.data\n",
      " |      MemoryMappedTable\n",
      " |      text: string\n",
      " |      label: int64\n",
      " |      ----\n",
      " |      text: [[\"compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\",\"the soundtrack alone is worth the price of admission .\",\"rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .\",\"beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .\",\"bielinsky is a filmmaker of impressive talent .\",\"so beautifully acted and directed , it's clear that washington most certainly has a new career ahead of him if he so chooses .\",\"a visual spectacle full of stunning images and effects .\",\"a gentle and engrossing character study .\",\"it's enough to watch huppert scheming , with her small , intelligent eyes as steady as any noir villain , and to enjoy the perfectly pitched web of tension that chabrol spins .\",\"an engrossing portrait of uncompromising artists trying to create something original against the backdrop of a corporate music industry that only seems to care about the bottom line .\",...,\"ultimately , jane learns her place as a girl , softens up and loses some of the intensity that made her an interesting character to begin with .\",\"ah-nuld's action hero days might be over .\",\"it's clear why deuces wild , which was shot two years ago , has been gathering dust on mgm's shelf .\",\"feels like nothing quite so much as a middle-aged moviemaker's attempt to surround himself with beautiful , half-naked women .\",\"when the precise nature of matthew's predicament finally comes into sharp focus , the revelation fails to justify the build-up .\",\"this picture is murder by numbers , and as easy to be bored by as your abc's , despite a few whopping shootouts .\",\"hilarious musical comedy though stymied by accents thick as mud .\",\"if you are into splatter movies , then you will probably have a reasonably good time with the salton sea .\",\"a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing indifference on the inner-city streets .\",\"the feature-length stretch . . . strains the show's concept .\"]]\n",
      " |      label: [[1,1,1,1,1,1,1,1,1,1,...,0,0,0,0,0,0,0,0,0,0]]\n",
      " |      ```\n",
      " |  \n",
      " |  format\n",
      " |  \n",
      " |  num_columns\n",
      " |      Number of columns in the dataset.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.num_columns\n",
      " |      2\n",
      " |      ```\n",
      " |  \n",
      " |  num_rows\n",
      " |      Number of rows in the dataset (same as :meth:`Dataset.__len__`).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.num_rows\n",
      " |      1066\n",
      " |      ```\n",
      " |  \n",
      " |  shape\n",
      " |      Shape of the dataset (number of columns, number of rows).\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> from datasets import load_dataset\n",
      " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
      " |      >>> ds.shape\n",
      " |      (1066, 2)\n",
      " |      ```\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from DatasetInfoMixin:\n",
      " |  \n",
      " |  builder_name\n",
      " |  \n",
      " |  citation\n",
      " |  \n",
      " |  config_name\n",
      " |  \n",
      " |  dataset_size\n",
      " |  \n",
      " |  description\n",
      " |  \n",
      " |  download_checksums\n",
      " |  \n",
      " |  download_size\n",
      " |  \n",
      " |  features\n",
      " |  \n",
      " |  homepage\n",
      " |  \n",
      " |  info\n",
      " |      :class:`datasets.DatasetInfo` object containing all the metadata in the dataset.\n",
      " |  \n",
      " |  license\n",
      " |  \n",
      " |  size_in_bytes\n",
      " |  \n",
      " |  split\n",
      " |      :class:`datasets.NamedSplit` object corresponding to a named dataset split.\n",
      " |  \n",
      " |  supervised_keys\n",
      " |  \n",
      " |  task_templates\n",
      " |  \n",
      " |  version\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DatasetInfoMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from datasets.search.IndexableMixin:\n",
      " |  \n",
      " |  drop_index(self, index_name: str)\n",
      " |      Drop the index with the specified column.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index.\n",
      " |  \n",
      " |  get_index(self, index_name: str) -> datasets.search.BaseIndex\n",
      " |      List the index_name/identifiers of all the attached indexes.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): Index name.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`BaseIndex`\n",
      " |  \n",
      " |  get_nearest_examples(self, index_name: str, query: Union[str, <built-in function array>], k: int = 10) -> datasets.search.NearestExamplesResults\n",
      " |      Find the nearest examples in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index.\n",
      " |          query (:obj:`Union[str, np.ndarray]`): The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (:obj:`int`): The number of examples to retrieve.\n",
      " |      \n",
      " |      Returns:\n",
      " |          scores (:obj:`List[float]`): The retrieval scores of the retrieved examples.\n",
      " |          examples (:obj:`dict`): The retrieved examples.\n",
      " |  \n",
      " |  get_nearest_examples_batch(self, index_name: str, queries: Union[List[str], <built-in function array>], k: int = 10) -> datasets.search.BatchedNearestExamplesResults\n",
      " |      Find the nearest examples in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index.\n",
      " |          queries (:obj:`Union[List[str], np.ndarray]`): The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (:obj:`int`): The number of examples to retrieve per query.\n",
      " |      \n",
      " |      Returns:\n",
      " |          total_scores (`List[List[float]`): The retrieval scores of the retrieved examples per query.\n",
      " |          total_examples (`List[dict]`): The retrieved examples per query.\n",
      " |  \n",
      " |  is_index_initialized(self, index_name: str) -> bool\n",
      " |  \n",
      " |  list_indexes(self) -> List[str]\n",
      " |      List the colindex_nameumns/identifiers of all the attached indexes.\n",
      " |  \n",
      " |  load_elasticsearch_index(self, index_name: str, es_index_name: str, host: Optional[str] = None, port: Optional[int] = None, es_client: Optional[ForwardRef('Elasticsearch')] = None, es_index_config: Optional[dict] = None)\n",
      " |      Load an existing text index using ElasticSearch for fast retrieval.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index. This is the index name that is used to call `.get_nearest` or `.search`.\n",
      " |          es_index_name (:obj:`str`): The name of elasticsearch index to load.\n",
      " |          host (Optional :obj:`str`, defaults to localhost):\n",
      " |              host of where ElasticSearch is running\n",
      " |          port (Optional :obj:`str`, defaults to 9200):\n",
      " |              port of where ElasticSearch is running\n",
      " |          es_client (Optional :obj:`elasticsearch.Elasticsearch`):\n",
      " |              The elasticsearch client used to create the index if host and port are None.\n",
      " |          es_index_config (Optional :obj:`dict`):\n",
      " |              The configuration of the elasticsearch index.\n",
      " |              Default config is::\n",
      " |      \n",
      " |                  {\n",
      " |                      \"settings\": {\n",
      " |                          \"number_of_shards\": 1,\n",
      " |                          \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
      " |                      },\n",
      " |                      \"mappings\": {\n",
      " |                          \"properties\": {\n",
      " |                              \"text\": {\n",
      " |                                  \"type\": \"text\",\n",
      " |                                  \"analyzer\": \"standard\",\n",
      " |                                  \"similarity\": \"BM25\"\n",
      " |                              },\n",
      " |                          }\n",
      " |                      },\n",
      " |                  }\n",
      " |  \n",
      " |  load_faiss_index(self, index_name: str, file: Union[str, pathlib.PurePath], device: Union[int, List[int], NoneType] = None)\n",
      " |      Load a FaissIndex from disk.\n",
      " |      \n",
      " |      If you want to do additional configurations, you can have access to the faiss index object by doing\n",
      " |      `.get_index(index_name).faiss_index` to make it fit your needs.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index. This is the index_name that is used to\n",
      " |              call `.get_nearest` or `.search`.\n",
      " |          file (:obj:`str`): The path to the serialized faiss index on disk.\n",
      " |          device (Optional :obj:`Union[int, List[int]]`): If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
      " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
      " |  \n",
      " |  save_faiss_index(self, index_name: str, file: Union[str, pathlib.PurePath])\n",
      " |      Save a FaissIndex on disk.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n",
      " |          file (:obj:`str`): The path to the serialized faiss index on disk.\n",
      " |  \n",
      " |  search(self, index_name: str, query: Union[str, <built-in function array>], k: int = 10) -> datasets.search.SearchResults\n",
      " |      Find the nearest examples indices in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The name/identifier of the index.\n",
      " |          query (:obj:`Union[str, np.ndarray]`): The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (:obj:`int`): The number of examples to retrieve.\n",
      " |      \n",
      " |      Returns:\n",
      " |          scores (:obj:`List[List[float]`): The retrieval scores of the retrieved examples.\n",
      " |          indices (:obj:`List[List[int]]`): The indices of the retrieved examples.\n",
      " |  \n",
      " |  search_batch(self, index_name: str, queries: Union[List[str], <built-in function array>], k: int = 10) -> datasets.search.BatchedSearchResults\n",
      " |      Find the nearest examples indices in the dataset to the query.\n",
      " |      \n",
      " |      Args:\n",
      " |          index_name (:obj:`str`): The index_name/identifier of the index.\n",
      " |          queries (:obj:`Union[List[str], np.ndarray]`): The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
      " |          k (:obj:`int`): The number of examples to retrieve per query.\n",
      " |      \n",
      " |      Returns:\n",
      " |          total_scores (:obj:`List[List[float]`): The retrieval scores of the retrieved examples per query.\n",
      " |          total_indices (:obj:`List[List[int]]`): The indices of the retrieved examples per query.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from TensorflowDatasetMixin:\n",
      " |  \n",
      " |  to_tf_dataset(self, batch_size: int, columns: Union[str, List[str], NoneType] = None, shuffle: bool = False, collate_fn: Optional[Callable] = None, drop_remainder: bool = False, collate_fn_args: Optional[Dict[str, Any]] = None, label_cols: Union[str, List[str], NoneType] = None, prefetch: bool = True)\n",
      " |      Create a tf.data.Dataset from the underlying Dataset. This tf.data.Dataset will load and collate batches from\n",
      " |      the Dataset, and is suitable for passing to methods like model.fit() or model.predict(). The dataset will yield\n",
      " |      dicts for both inputs and labels unless the dict would contain only a single key, in which case a raw\n",
      " |      tf.Tensor is yielded instead.\n",
      " |      \n",
      " |      Args:\n",
      " |          batch_size (:obj:`int`): Size of batches to load from the dataset.\n",
      " |          columns (:obj:`List[str]` or :obj:`str`, optional): Dataset column(s) to load in the tf.data.Dataset. Column\n",
      " |           names that are created by the `collate_fn` and that do not exist in the original dataset can be used.\n",
      " |          shuffle(:obj:`bool`, default to `False`): Shuffle the dataset order when loading. Recommended True for training, False for\n",
      " |              validation/evaluation.\n",
      " |          drop_remainder(:obj:`bool`, default ``False``): Drop the last incomplete batch when loading. Ensures\n",
      " |              that all batches yielded by the dataset will have the same length on the batch dimension.\n",
      " |          collate_fn(:obj:`Callable`, optional): A function or callable object (such as a `DataCollator`) that will collate\n",
      " |              lists of samples into a batch.\n",
      " |          collate_fn_args (:obj:`Dict`, optional): An optional `dict` of keyword arguments to be passed to the\n",
      " |              `collate_fn`.\n",
      " |          label_cols (:obj:`List[str]` or :obj:`str`, default ``None``): Dataset column(s) to load as\n",
      " |              labels. Note that many models compute loss internally rather than letting Keras do it, in which case\n",
      " |              passing the labels here is optional, as long as they're in the input `columns`.\n",
      " |          prefetch (:obj:`bool`, default ``True``): Whether to run the dataloader in a separate thread and maintain\n",
      " |              a small buffer of batches for training. Improves performance by allowing data to be loaded in the\n",
      " |              background while the model is training.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`tf.data.Dataset`\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```py\n",
      " |      >>> ds_train = ds[\"train\"].to_tf_dataset(\n",
      " |      ...    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
      " |      ...    shuffle=True,\n",
      " |      ...    batch_size=16,\n",
      " |      ...    collate_fn=data_collator,\n",
      " |      ... )\n",
      " |      ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ds[\"train\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
